import os
import streamlit as st
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
# from langchain.prompts.prompt import PromptTemplate
# from langchain.callbacks.manager import CallbackManager
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain.agents import AgentType, initialize_agent
from langchain_community.agent_toolkits.load_tools import load_tools
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.chains.question_answering import load_qa_chain
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from dotenv import load_dotenv
load_dotenv()

QA_PROMPT = """åªèƒ½ä½¿ç”¨ä»¥ä¸‹å…§å®¹ä¾†å›ç­”æœ€å¾Œçš„å•é¡Œï¼Œä¸¦éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè«‹ä¸è¦ç·¨é€ ç­”æ¡ˆã€‚åªéœ€èªª **æ‰¾ä¸åˆ°æœ€çµ‚ç­”æ¡ˆ**ã€‚
2. å¦‚æœæ‰¾åˆ°ç­”æ¡ˆï¼Œè«‹å‹™å¿…è©³ç´°åœ°æ­£ç¢ºå›ç­”ï¼Œä¸¦é™„ä¸Š**ç›´æ¥**ç”¨ä¾†å¾—å‡ºç­”æ¡ˆçš„ä¾†æºåˆ—è¡¨ã€‚æ’é™¤èˆ‡æœ€çµ‚ç­”æ¡ˆç„¡é—œçš„ä¾†æºã€‚
3. å¦‚æœscoreæœ€é«˜çš„å…§å®¹åŒ…å« Markdown æ ¼å¼çš„ ![]()ï¼Œè«‹åœ¨å›æ‡‰ä¸­ä¿ç•™å®ƒå®Œæ•´çš„æ¨£è²Œï¼ˆä¸€æ¨¡ä¸€æ¨£ï¼‰ã€‚
4. å®Œå…¨ä½¿ç”¨ç¹é«”ä¸­æ–‡é€²è¡Œæºé€šã€‚

{context}

å•é¡Œï¼š{question}
æœ‰å¹«åŠ©çš„å›ç­”ï¼š"""

car_model_list = ["CrossOver", "Delight", "SuperSport"]
car_model = "DELIGHT"
retriever_k = 5

st_callback = StreamlitCallbackHandler(st.container())
class StreamHandler(BaseCallbackHandler):
    """
    Callback handler to stream the generated text to Streamlit.
    Callback handler ç”¨æ–¼å°‡ç”Ÿæˆçš„æ–‡æœ¬æµå¼å‚³è¼¸åˆ° Streamlitã€‚
    """

    def __init__(self, container: st.container, initial_text: str="") -> None:
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        Append the new token to the text and update the Streamlit container.
        å°‡æ–°ä»¤ç‰Œé™„åŠ åˆ°æ–‡æœ¬ä¸¦æ›´æ–° Streamlit å®¹å™¨ã€‚
        """
        self.text += token
        self.container.markdown(self.text, unsafe_allow_html=True)

llm = ChatNVIDIA(
    model="meta/llama3-70b-instruct", 
    # model="nvidia/nemotron-4-340b-instruct", 
    nvidia_api_key=os.getenv("NVIDIA_API_KEY"), 
    temperature=0.0,
    top_p=1,
    max_tokens=1024,
    streaming=True,
    verbose=True,
    callbacks=[StreamHandler(st.empty())]
)

embedding_model = NVIDIAEmbeddings(model="nvidia/nv-embed-v1")
embedding_path = "embed_documents/ALL"

# tools = load_tools([])
# agent = initialize_agent(
#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
# )
db = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)

def set_CAR_MODEL_From_Session():
    pass
    global CAR_MODEL
    CAR_MODEL = st.session_state["car_model"]
    print("[DEBUG] set_CAR_MODEL_From_Session", CAR_MODEL)

st.title("ğŸï¸ Gogoro Smart Scooter è¬äº‹é€š")
with st.container():
    col1, col2 = st.columns(2)
    with col1:
        car_model = st.selectbox(
            "è»Šå‹", 
            car_model_list,
            on_change=set_CAR_MODEL_From_Session
        )
        car_model = car_model.upper()
        st.session_state["car_model"] = car_model
    with col2:
        retriever_k = st.slider(
            "Retriever Top K",
            min_value=1,
            max_value=8,
            value=4,
            step=1,
        )

retriever = db.as_retriever(search_kwargs={"k": retriever_k, 'filter': {"car_model": car_model}})
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    chain_type_kwargs={
        "prompt": PromptTemplate(
            template=QA_PROMPT,
            input_variables=["context", "question"],
        ),
    },
    retriever=retriever,
    return_source_documents=True,
    input_key="question",
)

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        response = st.write_stream(
            qa.stream(
                {
                    "question": prompt
                }, 
                # {
                #     "callbacks": [StreamHandler(st.empty())]
                # }
            )
        )
# messages = [
#     SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªèƒ½å¹²çš„åŠ©æ‰‹."),
#     HumanMessage(content="è°èµ¢å¾—äº†2020å¹´çš„ä¸–ç•ŒèŒä¸šæ£’çƒå¤§èµ›?"),
#     AIMessage(content="æ´›æ‰çŸ¶é“å¥‡é˜Ÿåœ¨2020å¹´èµ¢å¾—äº†ä¸–ç•ŒèŒä¸šæ£’çƒå¤§èµ›å† å†›."),
#     HumanMessage(content="å®ƒåœ¨å“ªé‡Œä¸¾åŠçš„?")
# ]

# print(llm.invoke(messages))
